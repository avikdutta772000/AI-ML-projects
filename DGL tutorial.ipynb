{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc08604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,initializers,constraints\n",
    "import time\n",
    "import math\n",
    "import subprocess\n",
    "import string\n",
    "from nltk.corpus import words\n",
    "import dgl\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from statistics import mean\n",
    "import networkx as nx\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ad5b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_file = open('Features/stopwords.txt','r')\n",
    "stopwords = stopwords_file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3584ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec embedding\n",
    "from gensim.models import KeyedVectors\n",
    "# Load vectors directly from the file\n",
    "word2vec = KeyedVectors.load_word2vec_format('Features/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# word2vec['word'] to get the 300 dim word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38193b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_portion_missing_word2vec():\n",
    "    for i in range(train_query_df.shape[0]):\n",
    "        c=0\n",
    "        for w in train_doc_dict[train_query_df.iloc[i]['index']]['word'].values:\n",
    "            if w not in word2vec.key_to_index.keys():\n",
    "                c+=1\n",
    "        print(\"{} ----------  {}/{}\".format(c*100/train_doc_dict[train_query_df.iloc[i]['index']].shape[0],c,train_doc_dict[train_query_df.iloc[i]['index']].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a69199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_query_doc(index,column_names):\n",
    "    quer_doc_loc = 'Features/feature/{}.txt'.format(index)\n",
    "    query_doc=pd.read_csv(quer_doc_loc, sep=\" \", skiprows=1,names=column_names) # only place where num_of_docs is required\n",
    "    query_doc[column_names[1:]]=query_doc[column_names[1:]].fillna(query_doc.mode().iloc[0])\n",
    "    query_doc=query_doc.replace('-âˆž',0)\n",
    "    query_doc[column_names[1:]]=query_doc[column_names[1:]].astype(float)\n",
    "    query_doc = query_doc[~query_doc['word'].isin(stopwords)]\n",
    "    query_doc = query_doc[query_doc['word'].isin(words.words())]\n",
    "    \n",
    "    # to be done for word2vec model\n",
    "    query_doc = query_doc[query_doc['word'].isin(word2vec.key_to_index.keys())]\n",
    "    \n",
    "    query_doc.reset_index(inplace=True)\n",
    "    query_doc = query_doc.drop('index', axis=1)\n",
    "    return query_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4db206b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graph(g):\n",
    "    nx_G = g.to_networkx()\n",
    "    # Kamada-Kawaii layout usually looks pretty for arbitrary graphs\n",
    "    pos = nx.kamada_kawai_layout(nx_G)\n",
    "    nx.draw(nx_G, pos, with_labels=True, node_color=[[.7, .7, .7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d63d9f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_emb_fn(word, query_ser, query_term):\n",
    "    global word2vec\n",
    "    global num_of_docs\n",
    "    emb = word2vec[word]\n",
    "    quer_list = query_term.split(' ')\n",
    "    sim_query=sum([word2vec.similarity(word, w) for w in quer_list if w in word2vec.key_to_index.keys()])\n",
    "    siz = len([w for w in quer_list if w in word2vec.key_to_index.keys()])\n",
    "    sim_query = sim_query/(siz if siz>0 else 1)\n",
    "    doc_freq = query_ser['df']\n",
    "    term_freq = mean([query_ser['tf{}'.format(i+1)] for i in range(num_of_docs)])\n",
    "    inv_freq = query_ser['idf']\n",
    "    word_emb = np.append(emb,np.array([sim_query,doc_freq,term_freq,inv_freq]))\n",
    "    return tf.convert_to_tensor(word_emb, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25e1547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide on the threshold for filtering edges\n",
    "# hyperparameter\n",
    "thresh_edges = 0.75\n",
    "\n",
    "num_of_docs=10 # change this if number of documents retrieved by a query changes\n",
    "num_of_features=4\n",
    "agg_dim=int(np.sqrt(num_of_features)) # Hyperparameter\n",
    "\n",
    "# data preprocessing for query file\n",
    "quer_df_loc = 'Features/678-topics.txt' \n",
    "query_df=pd.read_csv(quer_df_loc,sep='=>',names=['index','query'], engine='python') \n",
    "column_names=['word']\n",
    "column_names=column_names+['tf{}'.format(i) for i in range(1,num_of_docs+1)]\n",
    "column_names=column_names+['idf']\n",
    "column_names=column_names+['dl{}'.format(i) for i in range(1,num_of_docs+1)]\n",
    "column_names=column_names+['df']\n",
    "\n",
    "# to remove the retrieved docs which are wrongly structured\n",
    "query_df = query_df[(query_df['index']!=312) & (query_df['index']!=348) & (query_df['index']!=424)]\n",
    "\n",
    "\n",
    "# list of dataframes for the vocab and feature of each word in a given query\n",
    "ret_docs=[ret_query_doc(index,column_names) for index in query_df['index'].values ]\n",
    "query_doc_dict={query_df.iloc[i]['index']:ret_docs[i] for i in range(len(ret_docs))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c970a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1587"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_doc_dict[301].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "801feaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(query_term, query_doc): # This function is specific to the condition when low similarity edges are pruned\n",
    "    global thresh_edges\n",
    "    src_node = [k for j in [[i]*(query_doc.shape[0]) for i in range(query_doc.shape[0])] for k in j]\n",
    "    dest_node = [i for i in range(query_doc.shape[0])]*query_doc.shape[0]\n",
    "    \n",
    "    node_emb = tf.convert_to_tensor([word_emb_fn(query_doc.iloc[i]['word'], \n",
    "                                                  query_doc.iloc[i], \n",
    "                                                  query_term) for i in range(query_doc.shape[0])], dtype=tf.float32)\n",
    "    valid_edges = [(i,j) for i,j in tqdm(zip(src_node, dest_node)) if -tf.keras.losses.cosine_similarity(node_emb[i], node_emb[j]).numpy() >= thresh_edges]\n",
    "                   #tf.reduce_sum(tf.multiply(tf.nn.l2_normalize(node_emb[i]), \n",
    "                    #                                                  tf.nn.l2_normalize(node_emb[j]))).numpy() >= thresh_edges]\n",
    "    # valid_edges is a list of tuples (src_node, dest_node)\n",
    "    new_src_node, new_dest_node = zip(*valid_edges)\n",
    "    g = dgl.graph((new_src_node, new_dest_node), num_nodes=query_doc.shape[0])\n",
    "    g.ndata['word'] = tf.convert_to_tensor(query_doc['word'].values)\n",
    "    #edge_weights = tf.convert_to_tensor([tf.keras.losses.cosine_similarity(node_emb[i], node_emb[j]).numpy() for i,j in zip(src_node, dest_node)])\n",
    "    #g.edata['weight'] = edge_weights\n",
    "    #g = dgl.remove_self_loop(g)\n",
    "    return (g, node_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13516eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6578d8b2b24e34bc01767a711e9939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5719e247d0d4f79b9ddeb544cfbd847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-5942259eb704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# query_graph_dict[i] returns the graph and feature vectors for query with index i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m query_graph_dict = {query_df.iloc[i]['index']: build_graph(query_df.iloc[i]['query'], query_doc_dict[query_df.iloc[i]['index']]) \n\u001b[0;32m----> 3\u001b[0;31m                     for i in tqdm(range(query_df.shape[0]))}\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-5942259eb704>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# query_graph_dict[i] returns the graph and feature vectors for query with index i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m query_graph_dict = {query_df.iloc[i]['index']: build_graph(query_df.iloc[i]['query'], query_doc_dict[query_df.iloc[i]['index']]) \n\u001b[0;32m----> 3\u001b[0;31m                     for i in tqdm(range(query_df.shape[0]))}\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-e781b7282001>\u001b[0m in \u001b[0;36mbuild_graph\u001b[0;34m(query_term, query_doc)\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                   \u001b[0mquery_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                   query_term) for i in range(query_doc.shape[0])], dtype=tf.float32)\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mvalid_edges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mthresh_edges\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                    \u001b[0;31m#tf.reduce_sum(tf.multiply(tf.nn.l2_normalize(node_emb[i]),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0;31m#                                                  tf.nn.l2_normalize(node_emb[j]))).numpy() >= thresh_edges]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-e781b7282001>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                   \u001b[0mquery_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                   query_term) for i in range(query_doc.shape[0])], dtype=tf.float32)\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mvalid_edges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mthresh_edges\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                    \u001b[0;31m#tf.reduce_sum(tf.multiply(tf.nn.l2_normalize(node_emb[i]),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0;31m#                                                  tf.nn.l2_normalize(node_emb[j]))).numpy() >= thresh_edges]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvp3/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvp3/lib/python3.6/site-packages/keras/losses.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(y_true, y_pred, axis)\u001b[0m\n\u001b[1;32m   1964\u001b[0m     \u001b[0mCosine\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1965\u001b[0m   \"\"\"\n\u001b[0;32m-> 1966\u001b[0;31m   \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1967\u001b[0m   \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvp3/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvp3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m                 instructions)\n\u001b[0;32m--> 549\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/venvp3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py\u001b[0m in \u001b[0;36ml2_normalize\u001b[0;34m(x, axis, epsilon, name, dim)\u001b[0m\n\u001b[1;32m    679\u001b[0m     \u001b[0msquare_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mx_inv_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquare_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_inv_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# query_graph_dict[i] returns the graph and feature vectors for query with index i\n",
    "query_graph_dict = {query_df.iloc[i]['index']: build_graph(query_df.iloc[i]['query'], query_doc_dict[query_df.iloc[i]['index']]) \n",
    "                    for i in tqdm(range(query_df.shape[0]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b33b6693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0fd195ef0447c7905c04806c6ab0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g, node_emb = build_graph(query_df.iloc[0]['query'], query_doc_dict[301])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc337785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=1587, num_edges=802645,\n",
      "      ndata_schemes={'word': Scheme(shape=(), dtype=tf.string)}\n",
      "      edata_schemes={})\n",
      "tf.Tensor([b'undermining' b'german' b'spoke' ... b'response' b'york' b'assured'], shape=(1587,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "print(g)\n",
    "print(g.ndata['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1355b48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['undermining',\n",
       " 'german',\n",
       " 'spoke',\n",
       " 'numerous',\n",
       " 'salary',\n",
       " 'basta',\n",
       " 'kidnap',\n",
       " 'persistent',\n",
       " 'prison',\n",
       " 'conceal',\n",
       " 'successfully',\n",
       " 'possess',\n",
       " 'spreading',\n",
       " 'competent',\n",
       " 'require',\n",
       " 'analysis',\n",
       " 'apparent',\n",
       " 'journalist',\n",
       " 'size',\n",
       " 'left',\n",
       " 'seizing',\n",
       " 'newspaper',\n",
       " 'accessible',\n",
       " 'role',\n",
       " 'independent',\n",
       " 'ahead',\n",
       " 'achieve',\n",
       " 'unparalleled',\n",
       " 'talking',\n",
       " 'example',\n",
       " 'result',\n",
       " 'gold',\n",
       " 'feeding',\n",
       " 'commission',\n",
       " 'hand',\n",
       " 'policy',\n",
       " 'c',\n",
       " 'committee',\n",
       " 'address',\n",
       " 'f',\n",
       " 'namely',\n",
       " 'h',\n",
       " 'union',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'soviet',\n",
       " 'p',\n",
       " 'sale',\n",
       " 'police',\n",
       " 'v',\n",
       " 'unprecedented',\n",
       " 'fifth',\n",
       " 'persistence',\n",
       " 'days',\n",
       " 'information',\n",
       " 'returned',\n",
       " 'adopted',\n",
       " 'posing',\n",
       " 'liberal',\n",
       " 'standard',\n",
       " 'acceptable',\n",
       " 'powered',\n",
       " 'unreasonably',\n",
       " 'reader',\n",
       " 'middleman',\n",
       " 'serious',\n",
       " 'export',\n",
       " 'murder',\n",
       " 'fantasy',\n",
       " 'pay',\n",
       " 'enormous',\n",
       " 'onto',\n",
       " 'gone',\n",
       " 'governmental',\n",
       " 'lack',\n",
       " 'treasury',\n",
       " 'success',\n",
       " 'authority',\n",
       " 'supposed',\n",
       " 'categorically',\n",
       " 'shocking',\n",
       " 'regional',\n",
       " 'applied',\n",
       " 'provided',\n",
       " 'remove',\n",
       " 'society',\n",
       " 'protectionist',\n",
       " 'liaison',\n",
       " 'live',\n",
       " 'effectiveness',\n",
       " 'roof',\n",
       " 'mobile',\n",
       " 'expose',\n",
       " 'explosion',\n",
       " 'service',\n",
       " 'permit',\n",
       " 'suitable',\n",
       " 'personnel',\n",
       " 'magazine',\n",
       " 'taking',\n",
       " 'offense',\n",
       " 'considered',\n",
       " 'immortal',\n",
       " 'favorable',\n",
       " 'investigatory',\n",
       " 'entire',\n",
       " 'approach',\n",
       " 'gains',\n",
       " 'myth',\n",
       " 'russia',\n",
       " 'decisive',\n",
       " 'thesis',\n",
       " 'reservation',\n",
       " 'struck',\n",
       " 'period',\n",
       " 'considering',\n",
       " 'understand',\n",
       " 'disrespect',\n",
       " 'slowly',\n",
       " 'reformed',\n",
       " 'million',\n",
       " 'strongly',\n",
       " 'foreseeable',\n",
       " 'assumed',\n",
       " 'philosophize',\n",
       " 'engaged',\n",
       " 'gun',\n",
       " 'blackmail',\n",
       " 'successor',\n",
       " 'save',\n",
       " 'protection',\n",
       " 'matter',\n",
       " 'restrict',\n",
       " 'top',\n",
       " 'native',\n",
       " 'violence',\n",
       " 'accordingly',\n",
       " 'particularly',\n",
       " 'question',\n",
       " 'bought',\n",
       " 'wishing',\n",
       " 'milieu',\n",
       " 'framework',\n",
       " 'regard',\n",
       " 'rose',\n",
       " 'capital',\n",
       " 'con',\n",
       " 'smuggling',\n",
       " 'marijuana',\n",
       " 'manner',\n",
       " 'theses',\n",
       " 'employee',\n",
       " 'academic',\n",
       " 'function',\n",
       " 'west',\n",
       " 'stock',\n",
       " 'representative',\n",
       " 'traffic',\n",
       " 'counting',\n",
       " 'law',\n",
       " 'heroin',\n",
       " 'accumulation',\n",
       " 'liability',\n",
       " 'basis',\n",
       " 'oncoming',\n",
       " 'senator',\n",
       " 'reconcile',\n",
       " 'rarely',\n",
       " 'basic',\n",
       " 'taxation',\n",
       " 'specialized',\n",
       " 'censure',\n",
       " 'unclear',\n",
       " 'combat',\n",
       " 'effective',\n",
       " 'parlance',\n",
       " 'times',\n",
       " 'extra',\n",
       " 'land',\n",
       " 'victim',\n",
       " 'importation',\n",
       " 'department',\n",
       " 'possibility',\n",
       " 'lane',\n",
       " 'direction',\n",
       " 'wounded',\n",
       " 'tied',\n",
       " 'penetration',\n",
       " 'chief',\n",
       " 'foremost',\n",
       " 'despite',\n",
       " 'doctor',\n",
       " 'industrial',\n",
       " 'crescent',\n",
       " 'conduct',\n",
       " 'wall',\n",
       " 'incident',\n",
       " 'caught',\n",
       " 'refusing',\n",
       " 'apparatus',\n",
       " 'inspectorate',\n",
       " 'led',\n",
       " 'pity',\n",
       " 'discussion',\n",
       " 'wherein',\n",
       " 'concern',\n",
       " 'press',\n",
       " 'worker',\n",
       " 'cargo',\n",
       " 'element',\n",
       " 'chaos',\n",
       " 'forecast',\n",
       " 'recognize',\n",
       " 'cartridge',\n",
       " 'reality',\n",
       " 'brutality',\n",
       " 'difference',\n",
       " 'offend',\n",
       " 'circle',\n",
       " 'memorandum',\n",
       " 'perfectly',\n",
       " 'probably',\n",
       " 'jurisdiction',\n",
       " 'document',\n",
       " 'constitutional',\n",
       " 'stooge',\n",
       " 'giving',\n",
       " 'chase',\n",
       " 'moment',\n",
       " 'unfitting',\n",
       " 'promote',\n",
       " 'found',\n",
       " 'routine',\n",
       " 'attack',\n",
       " 'worried',\n",
       " 'situation',\n",
       " 'creation',\n",
       " 'legislation',\n",
       " 'inclination',\n",
       " 'receive',\n",
       " 'pointless',\n",
       " 'participate',\n",
       " 'accused',\n",
       " 'theft',\n",
       " 'entirely',\n",
       " 'standing',\n",
       " 'speech',\n",
       " 'polish',\n",
       " 'insurance',\n",
       " 'surely',\n",
       " 'principle',\n",
       " 'medical',\n",
       " 'lie',\n",
       " 'voting',\n",
       " 'domestic',\n",
       " 'unable',\n",
       " 'paying',\n",
       " 'late',\n",
       " 'school',\n",
       " 'basically',\n",
       " 'immense',\n",
       " 'shoot',\n",
       " 'acquired',\n",
       " 'budget',\n",
       " 'advantageous',\n",
       " 'eastern',\n",
       " 'tenuous',\n",
       " 'adapt',\n",
       " 'doubt',\n",
       " 'develop',\n",
       " 'surprising',\n",
       " 'privileged',\n",
       " 'exaggerated',\n",
       " 'involved',\n",
       " 'warp',\n",
       " 'impossible',\n",
       " 'house',\n",
       " 'wave',\n",
       " 'weapon',\n",
       " 'ecology',\n",
       " 'painful',\n",
       " 'steady',\n",
       " 'convention',\n",
       " 'opposing',\n",
       " 'yes',\n",
       " 'center',\n",
       " 'advised',\n",
       " 'equal',\n",
       " 'inasmuch',\n",
       " 'fund',\n",
       " 'short',\n",
       " 'time',\n",
       " 'titled',\n",
       " 'reducing',\n",
       " 'finance',\n",
       " 'incur',\n",
       " 'corrosion',\n",
       " 'shadow',\n",
       " 'notorious',\n",
       " 'happy',\n",
       " 'program',\n",
       " 'punitive',\n",
       " 'authorized',\n",
       " 'golden',\n",
       " 'payment',\n",
       " 'deprive',\n",
       " 'struggle',\n",
       " 'deputy',\n",
       " 'roughshod',\n",
       " 'dossier',\n",
       " 'destroy',\n",
       " 'creative',\n",
       " 'preventive',\n",
       " 'latent',\n",
       " 'provide',\n",
       " 'passenger',\n",
       " 'light',\n",
       " 'apparently',\n",
       " 'standpoint',\n",
       " 'unfounded',\n",
       " 'niche',\n",
       " 'societal',\n",
       " 'valuable',\n",
       " 'doubled',\n",
       " 'enterprise',\n",
       " 'confirmed',\n",
       " 'triangle',\n",
       " 'linguistic',\n",
       " 'elite',\n",
       " 'rebound',\n",
       " 'wholesale',\n",
       " 'suburban',\n",
       " 'low',\n",
       " 'fuss',\n",
       " 'wealth',\n",
       " 'chain',\n",
       " 'desire',\n",
       " 'initial',\n",
       " 'unfortunately',\n",
       " 'permanent',\n",
       " 'fellow',\n",
       " 'organized',\n",
       " 'alliance',\n",
       " 'fiction',\n",
       " 'tell',\n",
       " 'improper',\n",
       " 'experience',\n",
       " 'upsurge',\n",
       " 'outlined',\n",
       " 'major',\n",
       " 'emphasis',\n",
       " 'funds',\n",
       " 'consider',\n",
       " 'day',\n",
       " 'potential',\n",
       " 'combination',\n",
       " 'obtain',\n",
       " 'repeatedly',\n",
       " 'contraband',\n",
       " 'logically',\n",
       " 'resist',\n",
       " 'particular',\n",
       " 'history',\n",
       " 'illegal',\n",
       " 'corps',\n",
       " 'twice',\n",
       " 'distrust',\n",
       " 'absolutely',\n",
       " 'dont',\n",
       " 'arbitrarily',\n",
       " 'insurmountable',\n",
       " 'percent',\n",
       " 'penetrate',\n",
       " 'banking',\n",
       " 'guarded',\n",
       " 'east',\n",
       " 'induced',\n",
       " 'maneuverability',\n",
       " 'introduction',\n",
       " 'process',\n",
       " 'restore',\n",
       " 'built',\n",
       " 'alternative',\n",
       " 'lecturer',\n",
       " 'crowded',\n",
       " 'alibi',\n",
       " 'dope',\n",
       " 'third',\n",
       " 'build',\n",
       " 'mean',\n",
       " 'earn',\n",
       " 'account',\n",
       " 'methodology',\n",
       " 'apartment',\n",
       " 'tolerate',\n",
       " 'unmasked',\n",
       " 'radioactive',\n",
       " 'parliament',\n",
       " 'disorganization',\n",
       " 'evaluation',\n",
       " 'path',\n",
       " 'trip',\n",
       " 'appreciate',\n",
       " 'qualitative',\n",
       " 'record',\n",
       " 'criminological',\n",
       " 'examining',\n",
       " 'hunting',\n",
       " 'strict',\n",
       " 'knowledge',\n",
       " 'happen',\n",
       " 'carefully',\n",
       " 'past',\n",
       " 'active',\n",
       " 'opportunity',\n",
       " 'court',\n",
       " 'easy',\n",
       " 'population',\n",
       " 'meddle',\n",
       " 'senior',\n",
       " 'atlantic',\n",
       " 'route',\n",
       " 'trying',\n",
       " 'millions',\n",
       " 'assigned',\n",
       " 'secluded',\n",
       " 'relative',\n",
       " 'attitude',\n",
       " 'treatment',\n",
       " 'compare',\n",
       " 'truckload',\n",
       " 'influential',\n",
       " 'comes',\n",
       " 'soil',\n",
       " 'characteristic',\n",
       " 'interior',\n",
       " 'involvement',\n",
       " 'adoption',\n",
       " 'distinctive',\n",
       " 'peter',\n",
       " 'integral',\n",
       " 'hearing',\n",
       " 'hotel',\n",
       " 'term',\n",
       " 'handiwork',\n",
       " 'mind',\n",
       " 'business',\n",
       " 'emergence',\n",
       " 'operational',\n",
       " 'staff',\n",
       " 'pistol',\n",
       " 'noticeable',\n",
       " 'assemble',\n",
       " 'transition',\n",
       " 'answer',\n",
       " 'stage',\n",
       " 'series',\n",
       " 'complicated',\n",
       " 'maximum',\n",
       " 'phenomena',\n",
       " 'added',\n",
       " 'sometimes',\n",
       " 'emergency',\n",
       " 'promise',\n",
       " 'robbery',\n",
       " 'hold',\n",
       " 'shifting',\n",
       " 'lookout',\n",
       " 'directorate',\n",
       " 'legal',\n",
       " 'club',\n",
       " 'forgotten',\n",
       " 'radius',\n",
       " 'possibly',\n",
       " 'reviving',\n",
       " 'planet',\n",
       " 'finally',\n",
       " 'enforcement',\n",
       " 'prerequisite',\n",
       " 'straw',\n",
       " 'regarding',\n",
       " 'month',\n",
       " 'acceptance',\n",
       " 'final',\n",
       " 'relatively',\n",
       " 'reckon',\n",
       " 'carrying',\n",
       " 'occasion',\n",
       " 'loan',\n",
       " 'analytical',\n",
       " 'session',\n",
       " 'importance',\n",
       " 'endless',\n",
       " 'load',\n",
       " 'nuclear',\n",
       " 'human',\n",
       " 'notice',\n",
       " 'sole',\n",
       " 'owner',\n",
       " 'procuracy',\n",
       " 'relax',\n",
       " 'screwdriver',\n",
       " 'monitor',\n",
       " 'permission',\n",
       " 'strictly',\n",
       " 'congest',\n",
       " 'material',\n",
       " 'overwhelming',\n",
       " 'peculiar',\n",
       " 'president',\n",
       " 'qualified',\n",
       " 'ransom',\n",
       " 'helping',\n",
       " 'unjustified',\n",
       " 'attempt',\n",
       " 'soon',\n",
       " 'thick',\n",
       " 'division',\n",
       " 'opium',\n",
       " 'executive',\n",
       " 'text',\n",
       " 'usually',\n",
       " 'weakening',\n",
       " 'inspection',\n",
       " 'fear',\n",
       " 'fired',\n",
       " 'suspected',\n",
       " 'sense',\n",
       " 'stall',\n",
       " 'classification',\n",
       " 'operative',\n",
       " 'heavy',\n",
       " 'field',\n",
       " 'underground',\n",
       " 'impression',\n",
       " 'baseball',\n",
       " 'whereby',\n",
       " 'status',\n",
       " 'acting',\n",
       " 'phenomenon',\n",
       " 'headed',\n",
       " 'government',\n",
       " 'legally',\n",
       " 'legislator',\n",
       " 'laundry',\n",
       " 'crime',\n",
       " 'stand',\n",
       " 'twenty',\n",
       " 'socially',\n",
       " 'health',\n",
       " 'meticulous',\n",
       " 'prospect',\n",
       " 'precinct',\n",
       " 'machine',\n",
       " 'able',\n",
       " 'efficiently',\n",
       " 'return',\n",
       " 'instance',\n",
       " 'palliative',\n",
       " 'practically',\n",
       " 'feel',\n",
       " 'main',\n",
       " 'arrangement',\n",
       " 'solution',\n",
       " 'infallibility',\n",
       " 'monopoly',\n",
       " 'opponent',\n",
       " 'dramatically',\n",
       " 'international',\n",
       " 'credit',\n",
       " 'regardless',\n",
       " 'intensive',\n",
       " 'textbook',\n",
       " 'confiscate',\n",
       " 'ideal',\n",
       " 'curbing',\n",
       " 'sooner',\n",
       " 'equipment',\n",
       " 'difficult',\n",
       " 'organizationally',\n",
       " 'establishment',\n",
       " 'sort',\n",
       " 'seriously',\n",
       " 'professor',\n",
       " 'task',\n",
       " 'transit',\n",
       " 'valley',\n",
       " 'sore',\n",
       " 'position',\n",
       " 'curtail',\n",
       " 'cocaine',\n",
       " 'dangerous',\n",
       " 'leadership',\n",
       " 'magnitude',\n",
       " 'physical',\n",
       " 'included',\n",
       " 'met',\n",
       " 'experienced',\n",
       " 'falling',\n",
       " 'supervisory',\n",
       " 'exactly',\n",
       " 'structure',\n",
       " 'discredit',\n",
       " 'freeing',\n",
       " 'regulation',\n",
       " 'complicate',\n",
       " 'due',\n",
       " 'evident',\n",
       " 'threat',\n",
       " 'rayon',\n",
       " 'coming',\n",
       " 'psychological',\n",
       " 'depend',\n",
       " 'danger',\n",
       " 'crash',\n",
       " 'cover',\n",
       " 'firm',\n",
       " 'reflect',\n",
       " 'bank',\n",
       " 'contribution',\n",
       " 'meaning',\n",
       " 'fire',\n",
       " 'extort',\n",
       " 'based',\n",
       " 'brokerage',\n",
       " 'beaten',\n",
       " 'received',\n",
       " 'substantial',\n",
       " 'quality',\n",
       " 'unique',\n",
       " 'except',\n",
       " 'ministry',\n",
       " 'switching',\n",
       " 'professionalism',\n",
       " 'nevertheless',\n",
       " 'seizure',\n",
       " 'poppy',\n",
       " 'superiority',\n",
       " 'respectable',\n",
       " 'fundamental',\n",
       " 'association',\n",
       " 'global',\n",
       " 'eliminate',\n",
       " 'believe',\n",
       " 'fragmentation',\n",
       " 'bases',\n",
       " 'unless',\n",
       " 'operating',\n",
       " 'free',\n",
       " 'destruction',\n",
       " 'dirty',\n",
       " 'middle',\n",
       " 'bewilderment',\n",
       " 'people',\n",
       " 'democracy',\n",
       " 'entrepreneur',\n",
       " 'transportation',\n",
       " 'stay',\n",
       " 'actually',\n",
       " 'appear',\n",
       " 'extensive',\n",
       " 'residential',\n",
       " 'afraid',\n",
       " 'associated',\n",
       " 'worldly',\n",
       " 'increasing',\n",
       " 'invite',\n",
       " 'operation',\n",
       " 'alarming',\n",
       " 'militia',\n",
       " 'stabilization',\n",
       " 'forced',\n",
       " 'suffering',\n",
       " 'rapid',\n",
       " 'seven',\n",
       " 'arena',\n",
       " 'independence',\n",
       " 'genesis',\n",
       " 'whenever',\n",
       " 'roughly',\n",
       " 'five',\n",
       " 'internecine',\n",
       " 'compromising',\n",
       " 'surprise',\n",
       " 'wont',\n",
       " 'owing',\n",
       " 'look',\n",
       " 'grounds',\n",
       " 'laboratory',\n",
       " 'hypothetical',\n",
       " 'flying',\n",
       " 'campaign',\n",
       " 'vital',\n",
       " 'conflict',\n",
       " 'allow',\n",
       " 'distillery',\n",
       " 'unlicensed',\n",
       " 'mass',\n",
       " 'rule',\n",
       " 'proper',\n",
       " 'jurisprudence',\n",
       " 'assistance',\n",
       " 'bats',\n",
       " 'common',\n",
       " 'themselves',\n",
       " 'guiltless',\n",
       " 'summary',\n",
       " 'bandit',\n",
       " 'recently',\n",
       " 'indeed',\n",
       " 'artificial',\n",
       " 'money',\n",
       " 'riches',\n",
       " 'comment',\n",
       " 'step',\n",
       " 'interregional',\n",
       " 'tribute',\n",
       " 'base',\n",
       " 'employ',\n",
       " 'transnational',\n",
       " 'profitability',\n",
       " 'trend',\n",
       " 'balcony',\n",
       " 'consist',\n",
       " 'relation',\n",
       " 'loss',\n",
       " 'compatible',\n",
       " 'enable',\n",
       " 'lost',\n",
       " 'safety',\n",
       " 'price',\n",
       " 'jeep',\n",
       " 'slavery',\n",
       " 'racketeering',\n",
       " 'briefly',\n",
       " 'reduced',\n",
       " 'embezzlement',\n",
       " 'agree',\n",
       " 'fail',\n",
       " 'word',\n",
       " 'favorite',\n",
       " 'stated',\n",
       " 'internal',\n",
       " 'civilized',\n",
       " 'preventively',\n",
       " 'interlocutor',\n",
       " 'secret',\n",
       " 'foundation',\n",
       " 'ourselves',\n",
       " 'foreign',\n",
       " 'naturally',\n",
       " 'mysteriously',\n",
       " 'fall',\n",
       " 'moreover',\n",
       " 'federal',\n",
       " 'collective',\n",
       " 'academy',\n",
       " 'established',\n",
       " 'delineate',\n",
       " 'include',\n",
       " 'boundless',\n",
       " 'alongside',\n",
       " 'agency',\n",
       " 'subunit',\n",
       " 'godfather',\n",
       " 'opinion',\n",
       " 'altogether',\n",
       " 'prefer',\n",
       " 'prepared',\n",
       " 'existence',\n",
       " 'fourfold',\n",
       " 'chamber',\n",
       " 'spite',\n",
       " 'misappropriation',\n",
       " 'spree',\n",
       " 'bottom',\n",
       " 'ill',\n",
       " 'disappearance',\n",
       " 'manufacture',\n",
       " 'covered',\n",
       " 'invariably',\n",
       " 'aforesaid',\n",
       " 'bribery',\n",
       " 'death',\n",
       " 'arrival',\n",
       " 'according',\n",
       " 'frequent',\n",
       " 'network',\n",
       " 'holding',\n",
       " 'public',\n",
       " 'spend',\n",
       " 'repay',\n",
       " 'quantity',\n",
       " 'criminology',\n",
       " 'grave',\n",
       " 'advantage',\n",
       " 'assistant',\n",
       " 'charging',\n",
       " 'shameless',\n",
       " 'instead',\n",
       " 'command',\n",
       " 'compulsory',\n",
       " 'stolen',\n",
       " 'trade',\n",
       " 'round',\n",
       " 'growth',\n",
       " 'communist',\n",
       " 'exception',\n",
       " 'temporary',\n",
       " 'fare',\n",
       " 'juridical',\n",
       " 'leading',\n",
       " 'impunity',\n",
       " 'urge',\n",
       " 'branch',\n",
       " 'building',\n",
       " 'via',\n",
       " 'setting',\n",
       " 'poor',\n",
       " 'understanding',\n",
       " 'silence',\n",
       " 'near',\n",
       " 'extortion',\n",
       " 'unlikely',\n",
       " 'dealing',\n",
       " 'virtually',\n",
       " 'mayor',\n",
       " 'fairly',\n",
       " 'economy',\n",
       " 'community',\n",
       " 'moratorium',\n",
       " 'airport',\n",
       " 'cultural',\n",
       " 'science',\n",
       " 'municipal',\n",
       " 'detain',\n",
       " 'happening',\n",
       " 'detail',\n",
       " 'shortly',\n",
       " 'appropriate',\n",
       " 'firmly',\n",
       " 'providing',\n",
       " 'throat',\n",
       " 'socioeconomic',\n",
       " 'immediately',\n",
       " 'subsequently',\n",
       " 'refuse',\n",
       " 'aspect',\n",
       " 'targeted',\n",
       " 'aggressive',\n",
       " 'behavior',\n",
       " 'close',\n",
       " 'agreed',\n",
       " 'linked',\n",
       " 'breathed',\n",
       " 'professionalization',\n",
       " 'federation',\n",
       " 'unlimited',\n",
       " 'pushing',\n",
       " 'stratum',\n",
       " 'authoritatively',\n",
       " 'gangsterism',\n",
       " 'communism',\n",
       " 'exploit',\n",
       " 'nine',\n",
       " 'evidence',\n",
       " 'republic',\n",
       " 'telltale',\n",
       " 'official',\n",
       " 'ethics',\n",
       " 'red',\n",
       " 'act',\n",
       " 'post',\n",
       " 'leave',\n",
       " 'swift',\n",
       " 'surrender',\n",
       " 'add',\n",
       " 'minister',\n",
       " 'wrote',\n",
       " 'jail',\n",
       " 'gather',\n",
       " 'consent',\n",
       " 'article',\n",
       " 'respect',\n",
       " 'mutual',\n",
       " 'underworld',\n",
       " 'unawares',\n",
       " 'unforeseeable',\n",
       " 'exchange',\n",
       " 'concussion',\n",
       " 'useful',\n",
       " 'unsolved',\n",
       " 'rifle',\n",
       " 'trust',\n",
       " 'income',\n",
       " 'private',\n",
       " 'scoop',\n",
       " 'technical',\n",
       " 'united',\n",
       " 'production',\n",
       " 'overnight',\n",
       " 'prevent',\n",
       " 'behave',\n",
       " 'precisely',\n",
       " 'previously',\n",
       " 'sorry',\n",
       " 'scope',\n",
       " 'responsible',\n",
       " 'irresponsible',\n",
       " 'democratic',\n",
       " 'capitalism',\n",
       " 'surveillance',\n",
       " 'bullet',\n",
       " 'colonel',\n",
       " 'cockroach',\n",
       " 'rich',\n",
       " 'objectively',\n",
       " 'noted',\n",
       " 'special',\n",
       " 'environment',\n",
       " 'family',\n",
       " 'career',\n",
       " 'ago',\n",
       " 'gangster',\n",
       " 'newly',\n",
       " 'reformation',\n",
       " 'punish',\n",
       " 'property',\n",
       " 'decline',\n",
       " 'succeeding',\n",
       " 'narrow',\n",
       " 'judge',\n",
       " 'incomprehensible',\n",
       " 'similar',\n",
       " 'viewpoint',\n",
       " 'ecological',\n",
       " 'extradite',\n",
       " 'distributed',\n",
       " 'tough',\n",
       " 'system',\n",
       " 'likelihood',\n",
       " 'spot',\n",
       " 'driven',\n",
       " 'animated',\n",
       " 'partial',\n",
       " 'aid',\n",
       " 'optimism',\n",
       " 'city',\n",
       " 'confident',\n",
       " 'doubling',\n",
       " 'drastic',\n",
       " 'local',\n",
       " 'defeat',\n",
       " 'vacuum',\n",
       " 'encourage',\n",
       " 'assertion',\n",
       " 'share',\n",
       " 'speak',\n",
       " 'incidentally',\n",
       " 'tender',\n",
       " 'thoroughly',\n",
       " 'tome',\n",
       " 'television',\n",
       " 'chairman',\n",
       " 'sharp',\n",
       " 'megalomania',\n",
       " 'future',\n",
       " 'urgently',\n",
       " 'preceding',\n",
       " 'body',\n",
       " 'links',\n",
       " 'bureau',\n",
       " 'net',\n",
       " 'read',\n",
       " 'below',\n",
       " 'touch',\n",
       " 'real',\n",
       " 'impending',\n",
       " 'unit',\n",
       " 'monthly',\n",
       " 'uranium',\n",
       " 'barrage',\n",
       " 'interfering',\n",
       " 'financial',\n",
       " 'duma',\n",
       " 'media',\n",
       " 'running',\n",
       " 'weakness',\n",
       " 'eve',\n",
       " 'negatively',\n",
       " 'construction',\n",
       " 'universal',\n",
       " 'declared',\n",
       " 'abroad',\n",
       " 'transport',\n",
       " 'specific',\n",
       " 'ladder',\n",
       " 'productive',\n",
       " 'engage',\n",
       " ...]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w.decode('utf-8') for w in g.ndata['word'].numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc061f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_list = [query_graph_dict[query_df.iloc[i]['index']] for i in range(query_df.shape[0])]\n",
    "\n",
    "# train-test split\n",
    "train_test_split_ratio = 0.9\n",
    "train_graph_dict = {query_df.iloc[i]['index']: graph_list[i] \n",
    "                    for i in range(int(train_test_split_ratio*query_df.shape[0]))}\n",
    "test_graph_dict = {query_df.iloc[i]['index']: graph_list[i] \n",
    "                    for i in range(int(query_df.shape[0]*train_test_split_ratio),query_df.shape[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ea39d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(keras.Model):\n",
    "    def __init__(self,out_dim, negative_slope, activation, bias):\n",
    "        super(GATLayer,self).__init__()\n",
    "        self.fc = layers.Dense(out_dim,use_bias=bias)\n",
    "        self.attn_fc = layers.Dense(1, use_bias=bias)\n",
    "        self.actv_layer = layers.Activation(activation) if activation is not None else None\n",
    "        self.negative_slope = negative_slope\n",
    "        self.bias= bias\n",
    "        \n",
    "    def edge_attention(self,edges):\n",
    "        z2 = tf.concat([edges.src['z'], edges.dst['z']], axis=1)\n",
    "        a = self.attn_fc(z2)\n",
    "        return {'e' : layers.LeakyReLU(alpha = self.negative_slope)(a)} # here alpha is a hyperparameter\n",
    "    \n",
    "    def message_func(self, edges):\n",
    "        return {'z' : edges.src['z'], 'e' : edges.data['e']}\n",
    "    \n",
    "    def reduce_func(self, nodes):\n",
    "        # reduce UDF for equation (3) & (4)\n",
    "        # equation (3)\n",
    "        alpha = keras.activations.softmax(nodes.mailbox['e'], axis=1)\n",
    "        # equation (4)\n",
    "        h = tf.math.reduce_sum(alpha * nodes.mailbox['z'], axis=1)\n",
    "        h = self.actv_layer(h) if self.actv_layer is not None else h\n",
    "        return {'h' : h}\n",
    "    \n",
    "    def call(self,g,h):\n",
    "        self.g = g \n",
    "        z = self.fc(h)\n",
    "        self.g.ndata['z'] = z\n",
    "        self.g.apply_edges(self.edge_attention)\n",
    "        self.g.update_all(self.message_func, self.reduce_func)\n",
    "        return self.g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4a09786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGATLayer(keras.Model):\n",
    "    def __init__(self, out_dim, num_heads, merge='cat', negative_slope = 0.3, activation = None, bias = False):\n",
    "        super(MultiHeadGATLayer, self).__init__()\n",
    "        self.heads = []\n",
    "        for i in range(num_heads):\n",
    "            self.heads.append(GATLayer(out_dim, negative_slope, activation, bias))\n",
    "        self.merge = merge\n",
    "\n",
    "    def call(self, g, h):\n",
    "        self.g = g\n",
    "        \n",
    "        head_outs = [attn_head(g,h) for attn_head in self.heads]\n",
    "        if self.merge == 'cat':\n",
    "            # concat on the output feature dimension (dim=1)\n",
    "            return tf.concat(head_outs, axis=1)\n",
    "        else:\n",
    "            # merge using average\n",
    "            return tf.math.reduce_mean(tf.stack(head_outs), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3df8235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
       "array([[-5.504841 ,  2.4977145, -3.627901 ],\n",
       "       [-6.3327165,  3.2701852, -4.5804763],\n",
       "       [-5.206317 ,  2.1945338, -2.7495675],\n",
       "       [-5.504841 ,  2.4977145, -3.627901 ]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GATLayer(3, 0.3, None, False)\n",
    "model(dgl.graph(([1,2,1,0],[0,1,3,2])), tf.constant([[1,2,3,4,6],[2,3,4,5,6],[3,4,5,6,7],[6,4,5,2,4]], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5998b6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_graph(dgl.graph(([1,2,3,0],[0,1,1,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "98e843d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_Network(keras.Model):\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 num_hidden,\n",
    "                 num_classes,\n",
    "                 heads,\n",
    "                 activation = None,\n",
    "                 feat_drop = None,\n",
    "                 attn_drop = None,\n",
    "                 negative_slope = 0.3,\n",
    "                 merge = 'cat',\n",
    "                 bias = False):\n",
    "        super(Policy_Network, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.gat_layers = []\n",
    "        self.activation = activation\n",
    "        # input projection (no residual)\n",
    "        self.gat_layers.append(MultiHeadGATLayer(\n",
    "            num_hidden, heads[0], merge,\n",
    "            negative_slope, self.activation, bias))\n",
    "        # hidden layers\n",
    "        for l in range(1, num_layers):\n",
    "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
    "            self.gat_layers.append(MultiHeadGATLayer(\n",
    "                num_hidden, heads[l], merge,\n",
    "                negative_slope, self.activation, bias))\n",
    "        # output projection\n",
    "        self.gat_layers.append(MultiHeadGATLayer(\n",
    "            num_classes, heads[-1], merge,\n",
    "            negative_slope, None, False))\n",
    "\n",
    "    def call(self, g, inputs):\n",
    "        self.g = g\n",
    "        h = inputs\n",
    "        for l in range(self.num_layers):\n",
    "            h = self.gat_layers[l](self.g, h)\n",
    "            h = tf.reshape(h, (h.shape[0], -1))\n",
    "        # output projection\n",
    "        logits = tf.reduce_mean(self.gat_layers[-1](self.g, h), axis=1)\n",
    "        logits = keras.activations.softmax(tf.expand_dims(logits, axis=-1), axis=0)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ae8e8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       "array([[0.24788539],\n",
       "       [0.20613079],\n",
       "       [0.29809842],\n",
       "       [0.24788539]], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Policy_Network(3,3,2,[4,3,2])\n",
    "(model(dgl.graph(([1,2,1,0],[0,1,3,2])), tf.constant([[1,2,3,4,5],[2,3,4,5,6],[3,4,5,6,7],[6,4,5,2,4]], dtype=tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4ddcea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# develop the above functions as class functions\n",
    "class GAT_Implementation:\n",
    "    \n",
    "    def __init__(self,K):\n",
    "        self.model=Policy_Network(num_layers = 3, # Hyperparameters of Policy Network\n",
    "                             num_hidden = 4,\n",
    "                             num_classes = 1,\n",
    "                             heads = [4, 3, 2],\n",
    "                             activation = None,\n",
    "                             feat_drop = None,\n",
    "                             attn_drop = None,\n",
    "                             negative_slope = 0.3,\n",
    "                             merge = 'cat',\n",
    "                             bias = False)\n",
    "        self.prev_eval=0.0\n",
    "        self.K=K # number of samples extracted\n",
    "    \n",
    "    \n",
    "    \n",
    "    def prob_dist_query(self,F):  # ---> R(Q,F;C,Omega)   F has shape : (num_of_words,num_of_docs,num_of_features)\n",
    "        R=self.model(F,training=True)\n",
    "        R = tf.keras.activations.softmax(R,axis=0)\n",
    "        return R # returns array of softmax p(w/Q) -- shape(num_of_words,1)\n",
    "    \n",
    "    def sampling_function(self,R): # shape of R : (number_of_words, 1)\n",
    "        \n",
    "        samples=tf.convert_to_tensor(np.random.choice(np.arange(len(self.vocabulary)),\n",
    "                                                      size=(self.K,),replace=False,p=R[:,0].numpy()))\n",
    "        #samples = tf.random.categorical(tf.math.log(tf.transpose(R).numpy()),self.K)[0]\n",
    "        R_dash = tf.gather(params = R[:,0], indices = samples)\n",
    "        R_dash=R_dash/tf.math.reduce_sum(R_dash)\n",
    "        return samples,R_dash # returns (tensor,tensor)\n",
    "    \n",
    "    def sampling_test_function(self,R):\n",
    "        samples = tf.math.top_k(R[:,0],k=self.K)\n",
    "        R_dash = samples.values\n",
    "        R_dash = R_dash/tf.math.reduce_sum(R_dash)\n",
    "        return samples.indices,R_dash\n",
    "        \n",
    "    def prob_MLE_fn(self,q,samples):\n",
    "        R_MLE=tf.convert_to_tensor([q.count(self.vocabulary[i]) for i in samples.numpy()],dtype=tf.float32)\n",
    "        return R_MLE/len(q.split()) # returns tensor\n",
    "\n",
    "    def interpolation_function(self,R_MLE,R_dash,alpha=0.5): # alpha is Hyperparamter (importance of word in the original query)\n",
    "        R_cap=alpha*R_MLE+(1-alpha)*R_dash\n",
    "        return R_cap # returns tensor   \n",
    "    \n",
    "    def retrieval(self, index, query, rf_query):\n",
    "        filename = \"query/query_ti\"\n",
    "        xml_file = open(filename, \"w\", encoding='utf8')\n",
    "        n = xml_file.write(rf_query)\n",
    "        xml_file.close()\n",
    "    \n",
    "        #running indri query\n",
    "        indrirunquery_command =  '/home/ir-group/indri-5.12/runquery/IndriRunQuery'\n",
    "        result = subprocess.Popen([indrirunquery_command, filename],\n",
    "                                  stdout=subprocess.PIPE,\n",
    "                                  stderr=subprocess.STDOUT)\n",
    "        stdout, stderr = result.communicate()\n",
    "        \n",
    "        res_file = \"query/query_rq\"\n",
    "        file1 = open(res_file, \"wb\")\n",
    "        n1 = file1.write(stdout)\n",
    "        file1.close()\n",
    "        \n",
    "        #performing TREC_eval\n",
    "        indritrec_command =  '/home/ir-group/indri-5.12/trec_eval.9.0/trec_eval'\n",
    "        trec_qrel = '/home/ir-group/indri-5.12/trec_eval.9.0/678_qrel'\n",
    "        result1 = subprocess.Popen([indritrec_command, trec_qrel, res_file],\n",
    "                                  stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "        stdout, stderr = result1.communicate()\n",
    "        \n",
    "        #saving MAP results\n",
    "        map_file = \"query/query_MAP\"\n",
    "        file2 = open(map_file, \"wb\")\n",
    "        n2 = file2.write(stdout)\n",
    "        file2.close()\n",
    "\n",
    "        #returning the MAP result for a query\n",
    "        file = open('query/query_MAP')\n",
    "        content = file.readlines()\n",
    "        #print(content)\n",
    "        split_l = content[5].split(\"\\t\")\n",
    "        print(\"MAP value: \", split_l[2])\n",
    "        MAP = split_l[2]\n",
    "        return MAP\n",
    "\n",
    "    def query_reformulation(self, word_sample_dict, index, query):\n",
    "        index_filepath = \"/home/ir-group/indri-5.12/trec678.index/\"\n",
    "        preamble_string = \"<parameters>\\n<index>\"+index_filepath+\"</index>\\n<runID>RML</runID>\\n<trecFormat>true</trecFormat>\\n<rule>method:dirichlet;mu:1500</rule>\\n<count>10</count>\\n<query>\\n<number>\" + str(index) + \"</number>\\n<text>\"\n",
    "        end_string = \"</text>\\n</query>\\n</parameters>\"\n",
    "        expansion = \"\"\n",
    "        ifac = 0.75\n",
    "        table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "        for key, value in word_sample_dict.items():\n",
    "#            print(key, \"and\", value)\n",
    "            expansion = expansion + str(value) + \" \" + key + \" \"\n",
    "            # Removing punctuations as Indri doesnt support these characters\n",
    "            term = expansion.translate(table) #removing punctuations\n",
    "            \n",
    "        reformulation = \"#weight( \" + str(ifac) + \" #combine(\" + query + \") \" + str(1-ifac) + \" #combine( \" + expansion + \"))\"\n",
    "        reformulated_query = preamble_string + \" \" + reformulation + \" \" + end_string\n",
    "        #print(reformulated_query)\n",
    "        return reformulated_query\n",
    "    \n",
    "    def eval_fn(self, word_sample_dict, index, query): # takes input the dictionary of size self.K which maps the word with its weight\n",
    "        \n",
    "        rf_query = self.query_reformulation(word_sample_dict, index, query)\n",
    "        MAP = self.retrieval(index, query, rf_query)\n",
    "        #print(\"MAP form eval\", MAP, \" \", type(MAP))\n",
    "        return tf.cast(float(MAP),dtype=tf.float32)\n",
    "            \n",
    "    def reward_fn(self,word_sample_dict, index, query):  # doubt regarding the eval(Q,t) - eval(Q,t-1), the use of prev_eval ??\n",
    "        t=self.eval_fn(word_sample_dict, index, query)\n",
    "        reward=t-self.prev_eval\n",
    "        self.prev_eval=t\n",
    "        return reward\n",
    "    \n",
    "    def cross_entropy_loss(self,R_dash,R,word_sample):\n",
    "        #R_temp=tf.convert_to_tensor([np.log(R[:,0].numpy()[i]) for i in word_sample.numpy()],dtype=tf.float32)\n",
    "        R_temp = tf.gather(params=tf.math.log(R[:,0]),indices = word_sample.numpy())\n",
    "        return tf.math.reduce_sum([i[0]*i[1] for i in zip(R_dash,R_temp)])\n",
    "    \n",
    "    def compute_loss_fn(self,X_batch): # X_batch is the dataframe containing 'index' and 'query' column for a given batch_size\n",
    "        L=tf.constant(0.,dtype=tf.float32)\n",
    "        for i in range(X_batch.shape[0]):\n",
    "            index=int(X_batch.iloc[i]['index'])\n",
    "            query = str(X_batch.iloc[i]['query'])\n",
    "            print('----------------- Running query {} : {} -----------------'.format(index, query))\n",
    "            g, node_emb = self.query_graph_dict[index]\n",
    "            self.vocabulary=np.array([w.decode('utf-8') for w in g.ndata['word'].numpy()])\n",
    "            R = self.model(g, node_emb)\n",
    "            word_sample,R_dash=self.sampling_function(R)\n",
    "            R_MLE=self.prob_MLE_fn(query,word_sample)\n",
    "            R_cap=self.interpolation_function(R_MLE,R_dash,alpha=0.6) \n",
    "            word_sample_dict = dict(zip([self.vocabulary[i] for i in word_sample],R_cap.numpy()))\n",
    "            print(\"-> {}\".format(word_sample_dict))\n",
    "            L=L-self.reward_fn(word_sample_dict, index, query)*self.cross_entropy_loss(R_dash,R,word_sample)\n",
    "        L = tf.convert_to_tensor(L,dtype=tf.float32)\n",
    "        return L\n",
    "    def train(self,query_df,query_graph_dict,epochs=10,batch_size=16):  # takes input as the queries and the documents retrieved by each query as a dict\n",
    "        # query_df is the dataframe containing 'index' and 'query' column\n",
    "        num_of_queries=query_df.shape[0] \n",
    "        self.query_graph_dict=query_graph_dict\n",
    "        num_epochs=epochs\n",
    "        optimizer=keras.optimizers.Adam(learning_rate = 0.001)\n",
    "        for epoch in range(num_epochs):\n",
    "            print('Start of training epoch : {}'.format(epoch))\n",
    "            for i in tqdm(range(int(num_of_queries/batch_size))):     \n",
    "                X_batch=query_df.iloc[i*batch_size:(i+1)*batch_size] # X_batch is a dataframe with 'index' and 'query' column\n",
    "                with tf.GradientTape() as tape:\n",
    "                    loss = self.compute_loss_fn(X_batch)\n",
    "                    print(\"Loss for epoch={} and batch={} : {}\".format(epoch,i+1,loss))   \n",
    "                gradients=tape.gradient(loss,self.model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "                \n",
    "            if num_of_queries%batch_size!=0:\n",
    "                X_batch=query_df.iloc[int(num_of_queries/batch_size)*batch_size:]\n",
    "                with tf.GradientTape() as tape:\n",
    "                    loss=self.compute_loss_fn(X_batch)\n",
    "                    print(\"Loss for epoch={} and batch=rem : {}\".format(epoch,loss))   \n",
    "                gradients=tape.gradient(loss,self.model.trainable_weights)\n",
    "                optimizer.apply_gradients(zip(gradients,self.model.trainable_weights))\n",
    "        \n",
    "    def test(self,query_df,query_graph_dict):\n",
    "        print('Testing the model : ')\n",
    "        num_of_queries = query_df.shape[0]\n",
    "        self.query_graph_dict = query_graph_dict\n",
    "        map_arr = []\n",
    "        for i in range(query_df.shape[0]):\n",
    "            query = query_df.iloc[i]['query']\n",
    "            index = query_df.iloc[i]['index']\n",
    "            g, node_emb = self.query_graph_dict[index]\n",
    "            self.vocabulary=np.array([w.decode('utf-8') for w in g.ndata['word'].numpy()])\n",
    "            R = self.model(g, node_emb)\n",
    "            word_sample, R_dash = self.sampling_test_function(R)\n",
    "            R_MLE=self.prob_MLE_fn(query,word_sample)\n",
    "            R_cap=self.interpolation_function(R_MLE,R_dash,alpha=0.6)\n",
    "            word_sample_dict = dict(zip([self.vocabulary[i] for i in word_sample],R_cap.numpy()))\n",
    "            print('query : {}'.format(query))\n",
    "            print(\"Reformalised query terms -> {}\".format(word_sample_dict))\n",
    "            MAP = (self.eval_fn(word_sample_dict, index, query)).numpy()\n",
    "            map_arr = map_arr + [MAP]\n",
    "        return map_arr     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a08b33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training epoch : 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a364addf90464e9eff8cbcb476f6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Running query 301 : international organized crime -----------------\n",
      "-> {'bigger': 0.10042323, 'optimism': 0.099876694, 'occupant': 0.09977258, 'maximum': 0.099927485}\n",
      "MAP value:  0.0056\n",
      "\n",
      "Loss for epoch=0 and batch=1 : 0.04133712127804756\n",
      "Start of training epoch : 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6ebc248ead4d8d9792aca99add382b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Running query 301 : international organized crime -----------------\n",
      "-> {'internecine': 0.09956037, 'ourselves': 0.09968721, 'substantially': 0.09961853, 'seven': 0.10113388}\n",
      "MAP value:  0.0049\n",
      "\n",
      "Loss for epoch=1 and batch=1 : -0.005160697735846043\n",
      "time elapsed in training the model : 0.04540722330411275 hrs\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "gat=GAT_Implementation(4) # pass K : the number of samples to extract for query reformulation\n",
    "#x = train_doc_dict[train_query_df.iloc[0]['index']]\n",
    "#temp = rml.model(tf.random.uniform(shape=[x.shape[0],num_of_docs,num_of_features]),training=False)\n",
    "#rml.model.load_weights('saved_weights.h5') # comment this statement to train the model from scratch\n",
    "gat.train(query_df.iloc[:1],{301:(g, node_emb)},epochs = 2, batch_size = 1)\n",
    "end=time.time()\n",
    "print('time elapsed in training the model : {} hrs'.format((end-start)/3600))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf001710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
