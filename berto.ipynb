{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7f5ee4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset oscar/unshuffled_deduplicated_or (download: 36.93 MiB, generated: 188.26 MiB, post-processed: Unknown size, total: 225.19 MiB) to /home/somnath-am/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_or/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920e9bb6a8c44ba68a00329112141b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd0895b05e8483b881b20af85149e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9998cd5f20204380a9c971a5323becd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/38.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/44230 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset oscar downloaded and prepared to /home/somnath-am/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_or/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda38b8e5ef042ed9a0c961c98146f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('oscar', 'unshuffled_deduplicated_or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ee1512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'text': 'ଭୁବନେଶ୍ୱର, ୨୭/୧– (ଓଡ଼ିଆ ପୁଅ) ସିପିଆଇ ଜାତୀୟ ପରିଷଦର ଆହ୍ୱାନକ୍ରମେ ଗତକାଲି ଜାନୁୟାରୀ ୨୬ ସାଧାରଣତନ୍ତ୍ର ଦିବସକୁ ଦେଶ ବ୍ୟାପୀ ସମ୍ବିଧାନ ସୁରକ୍ଷା ଦିବସ ରୂପେ ପାଳନ କରାଯାଇଛି।\\nଏହି ଉପଲକ୍ଷେ ସିପିଆଇର ରାଜ୍ୟ ଦପ୍ତର ଭଗବତୀ ଭବନଠାରେ ରାଜ୍ୟ ସମ୍ପାଦକ ଦିବାକର ନାୟକଙ୍କ ଦ୍ୱାରା ଜାତୀୟ ପତାକା ଉତ୍ତୋଳନ କରାଯିବା ପରେ ସିପିଆଇ ଓ ବିକେଏମ୍ୟୁ ପକ୍ଷରୁ ଏକ ଶୋଭାଯାତ୍ରା ବାହାରିଥିଲା। ଶୋଭାଯାତ୍ରାକାରୀମାନେ ରାଜମହଲ ଛକ ଅତିକ୍ରମ କରି ଏଜି ଛକଠାରେ ଥିବା ଭାରତୀୟ ସମ୍ବିଧାନର ପ୍ରଣେତା ବାବା ସାହେବ ଭୀମରାଓ ଆମ୍ବେଦକରଙ୍କ ପ୍ରତିମୂର୍ତ୍ତିରେ ପୁଷ୍ପମାଲ୍ୟ ଅର୍ପଣ କରିଥିଲେ।\\nଏହାପରେ ସେଠାରେ ବିକେଏମ୍ୟୁ ରାଜ୍ୟ ସମ୍ପାଦକ ସୁର ଜେନାଙ୍କ ସଭାପତିତ୍ୱରେ ଅନୁଷ୍ଠିତ ସମାରୋହରେ ସିପିଆଇର ରାଜ୍ୟ ସମ୍ପାଦକ ଦିବାକର ନାୟକ, ସହସମ୍ପାଦକ ଆଶିଷ କାନୁନ୍ଗୋ, ଜାତୀୟ ପରିଷଦ ସଭ୍ୟ ରାମକୃଷ୍ଣ ପଣ୍ଡା ଓ ରାଜ୍ୟ ସମ୍ପାଦକ ମଣ୍ଡଳୀ ସଭ୍ୟ କ୍ଷୀରୋଦ ସିଂହଦେଓ ଉଦବୋଧନ ଦେଇ ମୋଦିଙ୍କ ନେତୃତ୍ୱରେ ପରିଚାଳିତ ବିଜେପି ସରକାରର ଛତ୍ରଛାୟା ତଳେ ସଂଘ ପରିବାର ନେତୃତ୍ୱରେ ସାମ୍ପ୍ରଦାୟିକ ଶକ୍ତିମାନେ ଭାରତୀୟ ସମ୍ବିଧାନର ମୂଳ ଭିତ୍ତି ଧର୍ମ ନିରପେକ୍ଷତା, ସଂସଦୀୟ ଗଣତନ୍ତ୍ର ଓ ସଙ୍ଘୀୟ ବ୍ୟବସ୍ଥାକୁ ବିପନ୍ନ କରି ଦେଶ ବ୍ୟାପୀ ଯେଉଁ ସାମ୍ପ୍ରଦାୟିକ ଅସହିଷ୍ଣୁତା, ଘୃଣା ଓ ହିଂସାର ବାତାବରଣ ସୃଷ୍ଟି କରିଚାଲିଛନ୍ତି, ତାହା ବିରୋଧରେ ଦେଶର ସମ୍ବିଧାନର ସୁରକ୍ଷା ପାଇଁ ବାମପନ୍ଥୀ, ଗଣତାନ୍ତ୍ରିକ ଓ ଧର୍ମନିରପେକ୍ଷ ଶକ୍ତିମାନେ ଐକ୍ୟବଦ୍ଧ ହେବା ପାଇଁ ଆହ୍ୱାନ ଦେଇଥିଲେ।\\nଅନ୍ୟମାନଙ୍କ ମଧ୍ୟରେ ସିପିଆଇ ଖୋର୍ଦ୍ଧା ଜିଲ୍ଲା ସହସମ୍ପାଦକ ସତୀଶ ମିଶ୍ର, ଭୁବନେଶ୍ୱର ଜୋନ୍ ସହସମ୍ପାଦକ କୈଳାସ ଚନ୍ଦ୍ର ପୁହାଣ, ଯୁବସଂଘ ନେତା ନାରାୟଣ ଡାଳୁଆ ଓ ସିପିଆଇର ଛେତମ୍ବର ଭୋଇ, ଲୋକନାଥ ଭୋଇ, ବାସନ୍ତୀ ପ୍ରଧାନ, ରୀତା ସେଠୀ ଓ ପୂଜା ମିଶ୍ର ପ୍ରମୁଖ ଶୋଭାଯାତ୍ରାର ନେତୃତ୍ୱ ନେଇଥିଲେ।'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a68ec06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir('oscar_or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7d866b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a048c084e2b474aa336c1b5a5c7533f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(dataset['train']):\n",
    "    sample = sample['text'].replace('\\n', ' ')\n",
    "    text_data.append(sample)\n",
    "    if len(text_data) == 10_000:\n",
    "        # once we git the 10K mark, save to file\n",
    "        with open(f'oscar_or/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "# after saving in 10K chunks, we will have ~2082 leftover samples, we save those now too\n",
    "with open(f'oscar_or/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead219ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "paths = [str(x) for x in Path('oscar_or').glob('*.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d69e9dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "#tokenizer = ByteLevelBPETokenizer()\n",
    "#tokenizer.train(files=paths, vocab_size=30000, min_frequency=2,\n",
    "#                special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# initialize\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=False,\n",
    "    strip_accents=False,\n",
    "    lowercase=False\n",
    ")\n",
    "# and train\n",
    "tokenizer.train(files=paths, vocab_size=50000, min_frequency=2,\n",
    "                limit_alphabet=1000, wordpieces_prefix='##',\n",
    "                special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3a9f820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['orberto/vocab.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import os\n",
    "\n",
    "#os.mkdir('orberto')\n",
    "\n",
    "tokenizer.save_model('orberto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8da41819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start running code from here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfa1a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "paths = [str(x) for x in Path('oscar_or').glob('*.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "171c23db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('orberto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f783304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def mlm(tensor):\n",
    "    rand = torch.rand(tensor.shape)\n",
    "    mask_arr = (rand<0.15)*(tensor>2)\n",
    "    for i in range(tensor.shape[0]):\n",
    "        selection = torch.flatten(mask_arr[i].nonzero())\n",
    "        tensor[i, selection] = 4\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d708094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33b4483077f47c4a3eefd81fea768fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "input_ids = []\n",
    "mask = []\n",
    "labels = []\n",
    "\n",
    "for path in tqdm(paths):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "    sample = tokenizer(lines, max_length = 512, padding='max_length', \n",
    "                       truncation=True, return_tensors = 'pt')\n",
    "    labels.append(sample.input_ids)\n",
    "    mask.append(sample.attention_mask)\n",
    "    input_ids.append(mlm(sample.input_ids.detach().clone())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faf8f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.cat(input_ids)\n",
    "mask = torch.cat(mask)\n",
    "labels = torch.cat(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bf8f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = {'input_ids': input_ids, \n",
    "            'attention_mask':mask,\n",
    "            'labels':labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d8c3e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __len__(self):\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return {key:tensor[i] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7d3bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8ed2e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=6, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1674498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2288976",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig(\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    max_position_embeddings = 514, # max length +2 (<s> and </s> should be included)\n",
    "    hidden_size = 768, \n",
    "    num_attention_heads = 12,\n",
    "    num_hidden_layers = 12,\n",
    "    type_vocab_size = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c4f9d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c07419b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForMaskedLM(config)\n",
    "#model = RobertaForMaskedLM.from_pretrained('orberto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9093a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20f8c564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=30000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61aaadc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "model.train()\n",
    "optim = AdamW(model.parameters(), lr = 1e-4, no_deprecation_warning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc01df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014e824f0bba4556a405b8e1b0e49e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7372 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8530d54c774cda946c5e2d7c434b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7372 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9a3309292f46c19d13c4ad5cf77aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7372 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32437e7e53394892b5b894dab0897984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7372 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c0d7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./orberto') # to save the model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf3f5fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "fill = pipeline('fill-mask', model='orberto', tokenizer='orberto')\n",
    "# {fill.tokenizer.mask_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c8d45ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.13307876884937286,\n",
       "  'token': 1,\n",
       "  'token_str': '<pad>',\n",
       "  'sequence': 'ଦୁନିଆରେ ଅନେକ କିସମର ଲୋକ ଅଛନ୍ତି ତାରି ଭିତରୁ ଏମିତି\\nମଣିଷ ବି ଅଛନ୍ତି ସେଇ କଥା କୁ ନ ଦେଖିଲେ କିଏ\\nବିଶ୍ୱାସ ବି କରନ୍ତେ ବା କାହିଁକି ଯେ'},\n",
       " {'score': 0.08965242654085159,\n",
       "  'token': 264,\n",
       "  'token_str': 'ା',\n",
       "  'sequence': 'ଦୁନିଆରେ ଅନେକ କିସମର ଲୋକ ଅଛନ୍ତି ତାରି ଭିତରୁ ଏମିତି\\nମଣିଷ ବି ଅଛନ୍ତି ସେଇ କଥାା କୁ ନ ଦେଖିଲେ କିଏ\\nବିଶ୍ୱାସ ବି କରନ୍ତେ ବା କାହିଁକି ଯେ'},\n",
       " {'score': 0.08253849297761917,\n",
       "  'token': 266,\n",
       "  'token_str': '୍',\n",
       "  'sequence': 'ଦୁନିଆରେ ଅନେକ କିସମର ଲୋକ ଅଛନ୍ତି ତାରି ଭିତରୁ ଏମିତି\\nମଣିଷ ବି ଅଛନ୍ତି ସେଇ କଥା୍ କୁ ନ ଦେଖିଲେ କିଏ\\nବିଶ୍ୱାସ ବି କରନ୍ତେ ବା କାହିଁକି ଯେ'},\n",
       " {'score': 0.07774076610803604,\n",
       "  'token': 265,\n",
       "  'token_str': 'ି',\n",
       "  'sequence': 'ଦୁନିଆରେ ଅନେକ କିସମର ଲୋକ ଅଛନ୍ତି ତାରି ଭିତରୁ ଏମିତି\\nମଣିଷ ବି ଅଛନ୍ତି ସେଇ କଥାି କୁ ନ ଦେଖିଲେ କିଏ\\nବିଶ୍ୱାସ ବି କରନ୍ତେ ବା କାହିଁକି ଯେ'},\n",
       " {'score': 0.04437841847538948,\n",
       "  'token': 268,\n",
       "  'token_str': 'େ',\n",
       "  'sequence': 'ଦୁନିଆରେ ଅନେକ କିସମର ଲୋକ ଅଛନ୍ତି ତାରି ଭିତରୁ ଏମିତି\\nମଣିଷ ବି ଅଛନ୍ତି ସେଇ କଥାେ କୁ ନ ଦେଖିଲେ କିଏ\\nବିଶ୍ୱାସ ବି କରନ୍ତେ ବା କାହିଁକି ଯେ'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill(f'''ଦୁନିଆରେ ଅନେକ କିସମର ଲୋକ ଅଛନ୍ତି ତାରି ଭିତରୁ ଏମିତି\n",
    "ମଣିଷ ବି ଅଛନ୍ତି ସେଇ କଥା {fill.tokenizer.mask_token} କୁ ନ ଦେଖିଲେ କିଏ\n",
    "ବିଶ୍ୱାସ ବି କରନ୍ତେ ବା କାହିଁକି ଯେ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86e9f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "213f7c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3bfd1311885425f80a6c13d162dbc0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = ['''ଦୁନିଆରେ ଅନେକ କିସମର ଲୋକ ଅଛନ୍ତି ତାରି ଭିତରୁ ଏମିତି\n",
    "ମଣିଷ ବି ଅଛନ୍ତି ସେଇ କଥା ପ୍ରଶାନ୍ତକୁ କୁ ନ ଦେଖିଲେ କିଏ\n",
    "ବିଶ୍ୱାସ ବି କରନ୍ତେ ବା କାହିଁକି ଯେ''',\n",
    "              '''ଘର କଉଠି ? ଦେଖିବା କୁ ତ ଭଲ ପୋଷାକ ପତ୍ର ପିନ୍ଧିଛ ? ତୁମ\n",
    "ନା ଟା କ'ଣ କହିଲ? ଏକା ସାଙ୍ଗରେ ପ୍ରଶାନ୍ତ ମୁହଁରୁ ଏତେ\n",
    "ଗୁଡ଼ାଏ ପ୍ରଶ୍ନ ଶୁଣି ସେଇ ବ୍ୟକ୍ତି ଜଣକ କିଛି ସମୟ\n",
    "ନିସ୍ତବ୍ଧ ଭାବରେ ଗାଡିର ରିଙ୍ଗ ତାକୁ କପଡ଼ାରେ ଘଷୁଥାନ୍ତି\n",
    "୤ପ୍ରଶାନ୍ତ ଟିକିଏ ଚୁପ ହୋଇ ଦାଣ୍ଡ ପିଣ୍ଡାରେ ବସି ପଡ଼ିଲା''',\n",
    "              '''ନୂଆ କର ବ୍ୟବସ୍ଥା ଦ୍ରବ୍ୟ ଓ ସେବା କର ଜୁଲାଇ ୧ରେ ସାରା\n",
    "ଦେଶରେ ଲାଗୁ ହେବା ସହ ବଦଳିବ ବହୁ ଜିନିଷର ଦର ।''',\n",
    "              \"\"\"ଓଡ଼ିଶା ପୋଲିସ୍ ଓ ୟୁନିସେଫ୍ର ମିଳିତ ସହଯୋଗରେ ଶିଶୁ ଯୌନ\n",
    "ନିର୍ଯାତନା ବିରୋଧରେ ରାଜ୍ୟବ୍ୟାପୀ ଏକ ଗୋÂୀ ସଚେତନତା\n",
    "ଅଭିଯାନ ଆୟୋଜନ କରାଯାଉଛି ।\"\"\",\n",
    "              \"\"\"ଆହତ ଯାତ୍ରୀମାନଙ୍କୁ ତୁରନ୍ତ ଗୁମ୍ମା\n",
    "ସ୍ୱାସ୍ଥ୍ୟକେନ୍ଦ୍ର ନିଆଯିବା ପରେ ସେଠାରେ ଗିଡିୟଏନ\n",
    "ଗୁରୁଙ୍କର ମୃତୁ୍ୟ ଘଟିଥିଲା ।\"\"\",\n",
    "              \"\"\"ବାଲେଶ୍ୱର : ଉତ୍ତର ଓଡ଼ିଶା ପ୍ରସିଦ୍ଧ ଶୈବପୀଠ ବାବା\n",
    "ପଞ୍ଚଲିଙ୍ଗେଶ୍ୱର ଆଜି ବର୍ଷା ଜଳରେ ବୁଡିଯାଇଛି।\"\"\"]\n",
    "results = perplexity.compute(predictions=predictions, model_id='orberto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ff8b276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'perplexities': [132.39492797851562,\n",
       "  237.56332397460938,\n",
       "  102.2740478515625,\n",
       "  132.39271545410156,\n",
       "  124.22488403320312,\n",
       "  112.30973815917969],\n",
       " 'mean_perplexity': 140.19327290852866}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a60fc41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avik_env",
   "language": "python",
   "name": "avik_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
